{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25326a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training Module for Credit Default Risk Prediction\n",
    "=========================================================\n",
    "\n",
    "Purpose:\n",
    "    - Train multiple ML models (Logistic Regression, XGBoost)\n",
    "    - Perform hyperparameter optimization\n",
    "    - Comprehensive model evaluation with industry metrics\n",
    "    - Save trained models and metadata\n",
    "\n",
    "Key Metrics:\n",
    "    - AUC-ROC: Overall discrimination ability\n",
    "    - Gini Coefficient: (2 √ó AUC) - 1 (credit industry standard)\n",
    "    - KS Statistic: Max separation between default/non-default\n",
    "    - Precision/Recall: Balance for imbalanced data\n",
    "\n",
    "Author: Credit Risk Analytics Team\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_validate, \n",
    "    StratifiedKFold,\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14bcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42910db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model training configuration\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.25  # 25% of remaining 80% = 20% overall\n",
    "N_JOBS = -1  # Use all available cores\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Evaluation thresholds\n",
    "THRESHOLD_LOW_RISK = 0.10\n",
    "THRESHOLD_HIGH_RISK = 0.25\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def print_section(title: str, width: int = 80):\n",
    "    \"\"\"Print formatted section header.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * width)\n",
    "    print(title.center(width))\n",
    "    print(\"=\" * width + \"\\n\")\n",
    "\n",
    "\n",
    "def save_json(data: dict, filepath: Path):\n",
    "    \"\"\"Save dictionary to JSON file.\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2, default=str)\n",
    "    print(f\"‚úì Saved: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06ed321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING & SPLITTING\n",
    "# =============================================================================\n",
    "\n",
    "def load_processed_data() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Load feature-engineered dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X: features DataFrame, y: target Series)\n",
    "    \"\"\"\n",
    "    \n",
    "    print_section(\"LOADING PROCESSED DATA\")\n",
    "    \n",
    "    filepath = DATA_DIR / \"features_engineered.csv\"\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"‚úó Error: {filepath} not found\")\n",
    "        print(\"Please run src/3_feature_engineering.py first\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Separate features and target\n",
    "    if 'Default' not in df.columns:\n",
    "        print(\"‚úó Error: Target column 'Default' not found\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    X = df.drop('Default', axis=1)\n",
    "    y = df['Default']\n",
    "    \n",
    "    print(f\"‚úì Loaded data:\")\n",
    "    print(f\"  Features shape: {X.shape}\")\n",
    "    print(f\"  Target shape: {y.shape}\")\n",
    "    print(f\"  Default rate: {y.mean()*100:.2f}%\")\n",
    "    print(f\"  Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_stratified_split(\n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Create stratified train/validation/test splits.\n",
    "    \n",
    "    Strategy:\n",
    "    - 60% train\n",
    "    - 20% validation  \n",
    "    - 20% test\n",
    "    - Stratification maintains class balance\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    print_section(\"CREATING DATA SPLITS\")\n",
    "    \n",
    "    # First split: 80% train+val, 20% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: 75% train, 25% val (of the 80%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=VAL_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Verify splits\n",
    "    total_samples = len(X)\n",
    "    \n",
    "    print(f\"Total samples: {total_samples:,}\")\n",
    "    print(f\"\\nTrain set:\")\n",
    "    print(f\"  Samples: {len(X_train):,} ({len(X_train)/total_samples*100:.1f}%)\")\n",
    "    print(f\"  Default rate: {y_train.mean()*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nValidation set:\")\n",
    "    print(f\"  Samples: {len(X_val):,} ({len(X_val)/total_samples*100:.1f}%)\")\n",
    "    print(f\"  Default rate: {y_val.mean()*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTest set:\")\n",
    "    print(f\"  Samples: {len(X_test):,} ({len(X_test)/total_samples*100:.1f}%)\")\n",
    "    print(f\"  Default rate: {y_test.mean()*100:.2f}%\")\n",
    "    \n",
    "    # Save split info\n",
    "    split_info = {\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'test_size': TEST_SIZE,\n",
    "        'val_size': VAL_SIZE,\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'train_default_rate': float(y_train.mean()),\n",
    "        'val_default_rate': float(y_val.mean()),\n",
    "        'test_default_rate': float(y_test.mean()),\n",
    "    }\n",
    "    \n",
    "    save_json(split_info, DATA_DIR / \"data_splits.json\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b47479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e62b6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_logistic_regression(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    use_grid_search: bool = True\n",
    ") -> Tuple[LogisticRegression, Dict]:\n",
    "    \"\"\"\n",
    "    Train Logistic Regression with hyperparameter tuning.\n",
    "    \n",
    "    Why Logistic Regression?\n",
    "    - Industry standard in credit risk (Basel, IFRS 9)\n",
    "    - Fully interpretable coefficients\n",
    "    - Regulatory-friendly (transparent decisions)\n",
    "    - Fast training and inference\n",
    "    - Probabilistic outputs\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        use_grid_search: Whether to perform hyperparameter tuning\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained model, training metrics dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    print_section(\"TRAINING: LOGISTIC REGRESSION\")\n",
    "    \n",
    "    if use_grid_search:\n",
    "        print(\"‚è≥ Performing Grid Search for hyperparameter tuning...\")\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],  # Regularization strength\n",
    "            'penalty': ['l2'],  # L2 regularization\n",
    "            'solver': ['lbfgs', 'liblinear'],\n",
    "            'max_iter': [1000],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "        \n",
    "        # Grid search with cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            LogisticRegression(random_state=RANDOM_STATE),\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=N_JOBS,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        model = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"\\n‚úì Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"‚úì Best CV AUC: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # Use default parameters with class weighting\n",
    "        model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"‚úì Model trained with default parameters\")\n",
    "    \n",
    "    # Validation predictions\n",
    "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Performance:\")\n",
    "    print(f\"  AUC-ROC: {val_auc:.4f}\")\n",
    "    print(f\"  F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"  Precision: {val_precision:.4f}\")\n",
    "    print(f\"  Recall: {val_recall:.4f}\")\n",
    "    \n",
    "    training_info = {\n",
    "        'model_type': 'Logistic Regression',\n",
    "        'parameters': model.get_params(),\n",
    "        'val_auc': val_auc,\n",
    "        'val_f1': val_f1,\n",
    "        'val_precision': val_precision,\n",
    "        'val_recall': val_recall,\n",
    "        'training_samples': len(X_train),\n",
    "        'training_time': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return model, training_info\n",
    "\n",
    "\n",
    "def train_xgboost(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    use_grid_search: bool = False\n",
    ") -> Tuple[xgb.XGBClassifier, Dict]:\n",
    "    \"\"\"\n",
    "    Train XGBoost with early stopping and hyperparameter tuning.\n",
    "    \n",
    "    Why XGBoost?\n",
    "    - State-of-the-art gradient boosting\n",
    "    - Captures non-linear relationships automatically\n",
    "    - Handles feature interactions\n",
    "    - Built-in regularization\n",
    "    - Excellent on tabular data\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        use_grid_search: Whether to perform hyperparameter tuning\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained model, training metrics dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    print_section(\"TRAINING: XGBOOST\")\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    print(f\"Class imbalance ratio: {scale_pos_weight:.2f}:1\")\n",
    "    print(f\"Using scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    if use_grid_search:\n",
    "        print(\"\\n‚è≥ Performing Grid Search (this may take a while)...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [5, 6, 7],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'scale_pos_weight': [scale_pos_weight]\n",
    "        }\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            xgb.XGBClassifier(\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=N_JOBS,\n",
    "                eval_metric='logloss'\n",
    "            ),\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=1,  # XGBoost uses parallel internally\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        model = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"\\n‚úì Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"‚úì Best CV AUC: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # Use optimized default parameters\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS,\n",
    "            eval_metric='logloss',\n",
    "            early_stopping_rounds=20\n",
    "        )\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Model trained with early stopping\")\n",
    "        print(f\"‚úì Best iteration: {model.best_iteration}\")\n",
    "    \n",
    "    # Validation predictions\n",
    "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Performance:\")\n",
    "    print(f\"  AUC-ROC: {val_auc:.4f}\")\n",
    "    print(f\"  F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"  Precision: {val_precision:.4f}\")\n",
    "    print(f\"  Recall: {val_recall:.4f}\")\n",
    "    \n",
    "    training_info = {\n",
    "        'model_type': 'XGBoost',\n",
    "        'parameters': model.get_params(),\n",
    "        'val_auc': val_auc,\n",
    "        'val_f1': val_f1,\n",
    "        'val_precision': val_precision,\n",
    "        'val_recall': val_recall,\n",
    "        'training_samples': len(X_train),\n",
    "        'best_iteration': int(model.best_iteration) if hasattr(model, 'best_iteration') else None,\n",
    "        'training_time': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return model, training_info\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_ks_statistic(y_true: np.ndarray, y_pred_proba: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Kolmogorov-Smirnov statistic.\n",
    "    \n",
    "    KS = max(TPR - FPR) across all thresholds\n",
    "    \n",
    "    Industry interpretation:\n",
    "    - KS > 40%: Excellent model\n",
    "    - 30-40%: Good model\n",
    "    - 20-30%: Acceptable\n",
    "    - < 20%: Poor\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred_proba: Predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "        KS statistic (0-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    ks = max(tpr - fpr)\n",
    "    return ks\n",
    "\n",
    "\n",
    "def evaluate_model_comprehensive(\n",
    "    model,\n",
    "    model_name: str,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with industry metrics.\n",
    "    \n",
    "    Metrics calculated:\n",
    "    - AUC-ROC: Overall discrimination ability\n",
    "    - Gini: (2 √ó AUC) - 1 (credit industry standard)\n",
    "    - KS Statistic: Regulatory requirement\n",
    "    - Precision/Recall: Class balance metrics\n",
    "    - Confusion Matrix: Error analysis\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        model_name: Model identifier\n",
    "        X_val, y_val: Validation data\n",
    "        X_test, y_test: Test data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print_section(f\"COMPREHENSIVE EVALUATION: {model_name}\")\n",
    "    \n",
    "    # Get predictions for both sets\n",
    "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # VALIDATION SET METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    val_gini = 2 * val_auc - 1\n",
    "    val_ks = calculate_ks_statistic(y_val, y_val_pred_proba)\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # TEST SET METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "    test_gini = 2 * test_auc - 1\n",
    "    test_ks = calculate_ks_statistic(y_test, y_test_pred_proba)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CONFUSION MATRIX (Test Set)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PRINT RESULTS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"VALIDATION SET METRICS:\")\n",
    "    print(f\"  AUC-ROC:    {val_auc:.4f}\")\n",
    "    print(f\"  Gini:       {val_gini:.4f}\")\n",
    "    print(f\"  KS Stat:    {val_ks:.4f}\")\n",
    "    print(f\"  Accuracy:   {val_accuracy:.4f}\")\n",
    "    print(f\"  Precision:  {val_precision:.4f}\")\n",
    "    print(f\"  Recall:     {val_recall:.4f}\")\n",
    "    print(f\"  F1 Score:   {val_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nTEST SET METRICS:\")\n",
    "    print(f\"  AUC-ROC:    {test_auc:.4f}\")\n",
    "    print(f\"  Gini:       {test_gini:.4f}\")\n",
    "    print(f\"  KS Stat:    {test_ks:.4f}\")\n",
    "    print(f\"  Accuracy:   {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision:  {test_precision:.4f}\")\n",
    "    print(f\"  Recall:     {test_recall:.4f}\")\n",
    "    print(f\"  F1 Score:   {test_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nCONFUSION MATRIX (Test Set):\")\n",
    "    print(f\"  True Negatives:   {tn:,}\")\n",
    "    print(f\"  False Positives:  {fp:,}\")\n",
    "    print(f\"  False Negatives:  {fn:,}\")\n",
    "    print(f\"  True Positives:   {tp:,}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # BUSINESS INTERPRETATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\nBUSINESS INTERPRETATION:\")\n",
    "    \n",
    "    # Gini interpretation\n",
    "    if test_gini > 0.5:\n",
    "        gini_interp = \"Excellent - Strong discrimination\"\n",
    "    elif test_gini > 0.4:\n",
    "        gini_interp = \"Good - Acceptable discrimination\"\n",
    "    elif test_gini > 0.3:\n",
    "        gini_interp = \"Fair - Minimum acceptable\"\n",
    "    else:\n",
    "        gini_interp = \"Poor - Needs improvement\"\n",
    "    print(f\"  Gini ({test_gini:.3f}): {gini_interp}\")\n",
    "    \n",
    "    # KS interpretation\n",
    "    if test_ks > 0.4:\n",
    "        ks_interp = \"Excellent separation\"\n",
    "    elif test_ks > 0.3:\n",
    "        ks_interp = \"Good separation\"\n",
    "    elif test_ks > 0.2:\n",
    "        ks_interp = \"Acceptable separation\"\n",
    "    else:\n",
    "        ks_interp = \"Poor separation\"\n",
    "    print(f\"  KS ({test_ks:.3f}): {ks_interp}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # COMPILE METRICS DICTIONARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        \n",
    "        # Validation metrics\n",
    "        'val_auc': float(val_auc),\n",
    "        'val_gini': float(val_gini),\n",
    "        'val_ks': float(val_ks),\n",
    "        'val_accuracy': float(val_accuracy),\n",
    "        'val_precision': float(val_precision),\n",
    "        'val_recall': float(val_recall),\n",
    "        'val_f1': float(val_f1),\n",
    "        \n",
    "        # Test metrics\n",
    "        'test_auc': float(test_auc),\n",
    "        'test_gini': float(test_gini),\n",
    "        'test_ks': float(test_ks),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_f1': float(test_f1),\n",
    "        \n",
    "        # Confusion matrix\n",
    "        'test_tn': int(tn),\n",
    "        'test_fp': int(fp),\n",
    "        'test_fn': int(fn),\n",
    "        'test_tp': int(tp),\n",
    "        \n",
    "        # Interpretations\n",
    "        'gini_interpretation': gini_interp,\n",
    "        'ks_interpretation': ks_interp,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL PERSISTENCE\n",
    "# =============================================================================\n",
    "\n",
    "def save_models(lr_model, xgb_model, lr_metrics: Dict, xgb_metrics: Dict):\n",
    "    \"\"\"\n",
    "    Save trained models and metadata.\n",
    "    \n",
    "    Args:\n",
    "        lr_model: Trained Logistic Regression\n",
    "        xgb_model: Trained XGBoost\n",
    "        lr_metrics: LR evaluation metrics\n",
    "        xgb_metrics: XGB evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print_section(\"SAVING MODELS\")\n",
    "    \n",
    "    # Save Logistic Regression\n",
    "    lr_path = MODELS_DIR / \"logistic_regression_model.pkl\"\n",
    "    with open(lr_path, 'wb') as f:\n",
    "        pickle.dump(lr_model, f)\n",
    "    print(f\"‚úì Saved: {lr_path}\")\n",
    "    \n",
    "    # Save XGBoost (use native format)\n",
    "    xgb_path = MODELS_DIR / \"xgboost_model.json\"\n",
    "    xgb_model.save_model(str(xgb_path))\n",
    "    print(f\"‚úì Saved: {xgb_path}\")\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_df = pd.DataFrame([lr_metrics, xgb_metrics])\n",
    "    comparison_path = MODELS_DIR / \"model_comparison.csv\"\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    print(f\"‚úì Saved: {comparison_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'models': {\n",
    "            'logistic_regression': lr_metrics,\n",
    "            'xgboost': xgb_metrics\n",
    "        },\n",
    "        'best_model': 'xgboost' if xgb_metrics['test_auc'] > lr_metrics['test_auc'] else 'logistic_regression',\n",
    "        'performance': {\n",
    "            'auc_roc': max(lr_metrics['test_auc'], xgb_metrics['test_auc']),\n",
    "            'gini': max(lr_metrics['test_gini'], xgb_metrics['test_gini']),\n",
    "            'ks_statistic': max(lr_metrics['test_ks'], xgb_metrics['test_ks'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    save_json(metadata, MODELS_DIR / \"model_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e8e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                      PHASE 4: MODEL TRAINING & EVALUATION                      \n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "                             LOADING PROCESSED DATA                             \n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded data:\n",
      "  Features shape: (1347681, 43)\n",
      "  Target shape: (1347681,)\n",
      "  Default rate: 19.98%\n",
      "  Memory usage: 226.20 MB\n",
      "\n",
      "================================================================================\n",
      "                              CREATING DATA SPLITS                              \n",
      "================================================================================\n",
      "\n",
      "Total samples: 1,347,681\n",
      "\n",
      "Train set:\n",
      "  Samples: 808,608 (60.0%)\n",
      "  Default rate: 19.98%\n",
      "\n",
      "Validation set:\n",
      "  Samples: 269,536 (20.0%)\n",
      "  Default rate: 19.98%\n",
      "\n",
      "Test set:\n",
      "  Samples: 269,537 (20.0%)\n",
      "  Default rate: 19.98%\n",
      "‚úì Saved: ../data/processed/data_splits.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print_section(\"PHASE 4: MODEL TRAINING & EVALUATION\", width=80)\n",
    "\n",
    "# Step 1: Load data\n",
    "X, y = load_processed_data()\n",
    "\n",
    "# Step 2: Create splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = create_stratified_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9087b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         TRAINING: LOGISTIC REGRESSION                          \n",
      "================================================================================\n",
      "\n",
      "‚úì Model trained with default parameters\n",
      "\n",
      "Validation Performance:\n",
      "  AUC-ROC: 0.6652\n",
      "  F1 Score: 0.3923\n",
      "  Precision: 0.2843\n",
      "  Recall: 0.6323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Train Logistic Regression\n",
    "lr_model, lr_train_info = train_logistic_regression(\n",
    "    X_train, y_train, X_val, y_val, use_grid_search=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a76ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                               TRAINING: XGBOOST                                \n",
      "================================================================================\n",
      "\n",
      "Class imbalance ratio: 4.01:1\n",
      "Using scale_pos_weight: 4.01\n",
      "‚úì Model trained with early stopping\n",
      "‚úì Best iteration: 193\n",
      "\n",
      "Validation Performance:\n",
      "  AUC-ROC: 0.6777\n",
      "  F1 Score: 0.4027\n",
      "  Precision: 0.2936\n",
      "  Recall: 0.6409\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Train XGBoost\n",
    "xgb_model, xgb_train_info = train_xgboost(\n",
    "    X_train, y_train, X_val, y_val, use_grid_search=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5039df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                 COMPREHENSIVE EVALUATION: Logistic Regression                  \n",
      "================================================================================\n",
      "\n",
      "VALIDATION SET METRICS:\n",
      "  AUC-ROC:    0.6652\n",
      "  Gini:       0.3305\n",
      "  KS Stat:    0.2367\n",
      "  Accuracy:   0.6085\n",
      "  Precision:  0.2843\n",
      "  Recall:     0.6323\n",
      "  F1 Score:   0.3923\n",
      "\n",
      "TEST SET METRICS:\n",
      "  AUC-ROC:    0.6638\n",
      "  Gini:       0.3277\n",
      "  KS Stat:    0.2343\n",
      "  Accuracy:   0.6079\n",
      "  Precision:  0.2838\n",
      "  Recall:     0.6319\n",
      "  F1 Score:   0.3917\n",
      "\n",
      "CONFUSION MATRIX (Test Set):\n",
      "  True Negatives:   129,833\n",
      "  False Positives:  85,854\n",
      "  False Negatives:  19,822\n",
      "  True Positives:   34,028\n",
      "\n",
      "BUSINESS INTERPRETATION:\n",
      "  Gini (0.328): Fair - Minimum acceptable\n",
      "  KS (0.234): Acceptable separation\n",
      "\n",
      "================================================================================\n",
      "                       COMPREHENSIVE EVALUATION: XGBoost                        \n",
      "================================================================================\n",
      "\n",
      "VALIDATION SET METRICS:\n",
      "  AUC-ROC:    0.6777\n",
      "  Gini:       0.3554\n",
      "  KS Stat:    0.2566\n",
      "  Accuracy:   0.6202\n",
      "  Precision:  0.2936\n",
      "  Recall:     0.6409\n",
      "  F1 Score:   0.4027\n",
      "\n",
      "TEST SET METRICS:\n",
      "  AUC-ROC:    0.6764\n",
      "  Gini:       0.3528\n",
      "  KS Stat:    0.2533\n",
      "  Accuracy:   0.6191\n",
      "  Precision:  0.2924\n",
      "  Recall:     0.6383\n",
      "  F1 Score:   0.4010\n",
      "\n",
      "CONFUSION MATRIX (Test Set):\n",
      "  True Negatives:   132,492\n",
      "  False Positives:  83,195\n",
      "  False Negatives:  19,477\n",
      "  True Positives:   34,373\n",
      "\n",
      "BUSINESS INTERPRETATION:\n",
      "  Gini (0.353): Fair - Minimum acceptable\n",
      "  KS (0.253): Acceptable separation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Comprehensive evaluation\n",
    "lr_metrics = evaluate_model_comprehensive(\n",
    "    lr_model, \"Logistic Regression\", X_val, y_val, X_test, y_test\n",
    ")\n",
    "\n",
    "xgb_metrics = evaluate_model_comprehensive(\n",
    "    xgb_model, \"XGBoost\", X_val, y_val, X_test, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0962d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                                MODEL COMPARISON                                \n",
      "================================================================================\n",
      "\n",
      "         model_name  test_auc  test_gini  test_ks  test_f1\n",
      "Logistic Regression  0.663842   0.327684 0.234257 0.391730\n",
      "            XGBoost  0.676412   0.352823 0.253338 0.401043\n",
      "\n",
      "üèÜ Best Model: XGBoost (AUC: 0.6764)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 6: Model comparison\n",
    "print_section(\"MODEL COMPARISON\")\n",
    "comparison = pd.DataFrame([lr_metrics, xgb_metrics])\n",
    "print(comparison[['model_name', 'test_auc', 'test_gini', 'test_ks', 'test_f1']].to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "if xgb_metrics['test_auc'] > lr_metrics['test_auc']:\n",
    "    print(f\"\\nüèÜ Best Model: XGBoost (AUC: {xgb_metrics['test_auc']:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nüèÜ Best Model: Logistic Regression (AUC: {lr_metrics['test_auc']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f41585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                                 SAVING MODELS                                  \n",
      "================================================================================\n",
      "\n",
      "‚úì Saved: ../models/logistic_regression_model.pkl\n",
      "‚úì Saved: ../models/xgboost_model.json\n",
      "‚úì Saved: ../models/model_comparison.csv\n",
      "‚úì Saved: ../models/model_metadata.json\n",
      "\n",
      "================================================================================\n",
      "                               ‚úì PHASE 4 COMPLETE                               \n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "  ‚úì Models trained and validated\n",
      "  ‚úì Performance metrics calculated\n",
      "  ‚úì Models saved to: ../models\n",
      "\n",
      "Next steps:\n",
      "  1. Review model comparison: cat models/model_comparison.csv\n",
      "  2. Run explainability: python src/5_shap_analysis.py\n",
      "  3. Start API: python app/main.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Save models\n",
    "save_models(lr_model, xgb_model, lr_metrics, xgb_metrics)\n",
    "\n",
    "# Final summary\n",
    "print_section(\"‚úì PHASE 4 COMPLETE\", width=80)\n",
    "print(\"Summary:\")\n",
    "print(f\"  ‚úì Models trained and validated\")\n",
    "print(f\"  ‚úì Performance metrics calculated\")\n",
    "print(f\"  ‚úì Models saved to: {MODELS_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review model comparison: cat models/model_comparison.csv\")\n",
    "print(\"  2. Run explainability: python src/5_shap_analysis.py\")\n",
    "print(\"  3. Start API: python app/main.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820da85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
