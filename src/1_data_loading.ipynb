{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a292de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47dbf420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                       PHASE 1: DATA LOADING & VALIDATION                       \n",
      "================================================================================\n",
      "\n",
      "üîß Setting up directory structure...\n",
      "‚úì Ensured directory: ../data\n",
      "‚úì Ensured directory: ../data/raw\n",
      "‚úì Ensured directory: ../data/processed\n",
      "‚úì Data already exists at ../data/raw/lending_club_loans.csv\n",
      "  Size: 159.71 MB\n",
      "  Use force=True to re-download\n",
      "\n",
      "================================================================================\n",
      "                                  LOADING DATA                                  \n",
      "================================================================================\n",
      "\n",
      "‚è≥ Loading data from ../data/raw/lending_club_loans.csv...\n",
      "‚úì Loaded 1,347,681 rows √ó 15 columns\n",
      "\n",
      "================================================================================\n",
      "                                VALIDATING DATA                                 \n",
      "================================================================================\n",
      "\n",
      "‚úì Data validation passed!\n",
      "\n",
      "‚è≥ Analyzing data quality...\n",
      "\n",
      "================================================================================\n",
      "                                  DATA SUMMARY                                  \n",
      "================================================================================\n",
      "\n",
      "üìä BASIC INFORMATION:\n",
      "  Rows: 1,347,681\n",
      "  Columns: 15\n",
      "  Memory: 728.25 MB\n",
      "  Duplicates: 0\n",
      "\n",
      "üìã DATA TYPES:\n",
      "  Numeric: 7\n",
      "  Text/Object: 8\n",
      "  Datetime: 0\n",
      "\n",
      "üîç MISSING VALUES:\n",
      "  Total: 1,245,240\n",
      "  Percentage: 6.16%\n",
      "  Columns affected: 3\n",
      "\n",
      "üéØ TARGET DISTRIBUTION:\n",
      "  Fully Paid (0): 1,078,432 (80.02%)\n",
      "  Default (1): 269,249 (19.98%)\n",
      "  Default Rate: 19.98%\n",
      "\n",
      "üìù SAMPLE COLUMNS (first 10):\n",
      "   1. id                             (int64) - 1,347,681 non-null\n",
      "   2. issue_d                        (object) - 1,347,681 non-null\n",
      "   3. revenue                        (float64) - 1,347,681 non-null\n",
      "   4. dti_n                          (float64) - 1,347,681 non-null\n",
      "   5. loan_amnt                      (int64) - 1,347,681 non-null\n",
      "   6. fico_n                         (float64) - 1,347,681 non-null\n",
      "   7. experience_c                   (int64) - 1,347,681 non-null\n",
      "   8. emp_length                     (object) - 1,347,681 non-null\n",
      "   9. purpose                        (object) - 1,347,681 non-null\n",
      "  10. home_ownership_n               (object) - 1,347,681 non-null\n",
      "\n",
      "‚úì Metadata saved to: ../data/raw_data_metadata.json\n",
      "\n",
      "================================================================================\n",
      "                               ‚úì PHASE 1 COMPLETE                               \n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Review metadata: cat data/raw_data_metadata.json\n",
      "  2. Run EDA: python src/2_exploratory_analysis.py\n",
      "  3. Check quality report for any concerns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Loading Module for Credit Default Risk Prediction\n",
    "========================================================\n",
    "\n",
    "Purpose:\n",
    "    - Download LendingClub dataset from stable source (Zenodo)\n",
    "    - Validate data integrity and structure\n",
    "    - Perform initial quality checks\n",
    "    - Save metadata for reproducibility\n",
    "\n",
    "Author: Credit Risk Analytics Team\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset metadata\n",
    "DATASET_CONFIG = {\n",
    "    'url': 'https://zenodo.org/records/11295916/files/LC_loans_granting_model_dataset.csv',\n",
    "    'expected_rows_min': 1_000_000,\n",
    "    'expected_rows_max': 1_500_000,\n",
    "    'expected_columns_min': 10,\n",
    "    'expected_columns_max': 20,\n",
    "    'target_column': 'Default',\n",
    "    'expected_md5': None,  # Add if known for validation\n",
    "}\n",
    "\n",
    "# Directory structure\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# File paths\n",
    "RAW_DATA_PATH = RAW_DIR / \"lending_club_loans.csv\"\n",
    "METADATA_PATH = DATA_DIR / \"raw_data_metadata.json\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_directories():\n",
    "    \"\"\"Create required directory structure if not exists.\"\"\"\n",
    "    for directory in [DATA_DIR, RAW_DIR, PROCESSED_DIR]:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚úì Ensured directory: {directory}\")\n",
    "\n",
    "\n",
    "def calculate_md5(filepath: Path, chunk_size: int = 8192) -> str:\n",
    "    \"\"\"\n",
    "    Calculate MD5 hash of file for integrity verification.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to file\n",
    "        chunk_size: Size of chunks to read (default 8KB)\n",
    "    \n",
    "    Returns:\n",
    "        MD5 hash as hex string\n",
    "    \"\"\"\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "\n",
    "def format_bytes(bytes_val: int) -> str:\n",
    "    \"\"\"Convert bytes to human-readable format.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if bytes_val < 1024:\n",
    "            return f\"{bytes_val:.2f} {unit}\"\n",
    "        bytes_val /= 1024\n",
    "    return f\"{bytes_val:.2f} TB\"\n",
    "\n",
    "\n",
    "def print_section_header(title: str, width: int = 80):\n",
    "    \"\"\"Print formatted section header.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * width)\n",
    "    print(title.center(width))\n",
    "    print(\"=\" * width + \"\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA DOWNLOAD\n",
    "# =============================================================================\n",
    "\n",
    "def download_data(\n",
    "    url: str, \n",
    "    filepath: Path, \n",
    "    force: bool = False\n",
    ") -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Download dataset from URL with progress tracking and validation.\n",
    "    \n",
    "    Args:\n",
    "        url: Source URL\n",
    "        filepath: Destination file path\n",
    "        force: Force re-download if file exists\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success: bool, error_message: Optional[str])\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if file already exists\n",
    "    if filepath.exists() and not force:\n",
    "        print(f\"‚úì Data already exists at {filepath}\")\n",
    "        print(f\"  Size: {format_bytes(filepath.stat().st_size)}\")\n",
    "        print(f\"  Use force=True to re-download\")\n",
    "        return True, None\n",
    "    \n",
    "    print(f\"‚è≥ Downloading dataset from Zenodo...\")\n",
    "    print(f\"   URL: {url}\")\n",
    "    print(f\"   Destination: {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        # Make request with stream=True for large files\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Get total file size\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        if total_size == 0:\n",
    "            return False, \"Could not determine file size\"\n",
    "        \n",
    "        print(f\"   Total size: {format_bytes(total_size)}\")\n",
    "        print(f\"   Estimated time: ~2-5 minutes (depending on connection)\")\n",
    "        \n",
    "        # Download with progress bar\n",
    "        with open(filepath, 'wb') as f, tqdm(\n",
    "            total=total_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            desc='Downloading'\n",
    "        ) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        \n",
    "        print(f\"\\n‚úì Download complete!\")\n",
    "        print(f\"  File saved: {filepath}\")\n",
    "        print(f\"  Size: {format_bytes(filepath.stat().st_size)}\")\n",
    "        \n",
    "        # Calculate MD5 for integrity\n",
    "        print(f\"\\n‚è≥ Calculating file hash for integrity check...\")\n",
    "        md5_hash = calculate_md5(filepath)\n",
    "        print(f\"‚úì MD5 Hash: {md5_hash}\")\n",
    "        \n",
    "        return True, None\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        return False, \"Download timeout - please check your internet connection\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return False, f\"Download failed: {str(e)}\"\n",
    "    \n",
    "    except IOError as e:\n",
    "        return False, f\"File write error: {str(e)}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_data_structure(df: pd.DataFrame) -> Tuple[bool, list]:\n",
    "    \"\"\"\n",
    "    Validate dataset structure and basic quality.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to validate\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (is_valid: bool, errors: list)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check row count\n",
    "    if not (DATASET_CONFIG['expected_rows_min'] <= len(df) <= DATASET_CONFIG['expected_rows_max']):\n",
    "        errors.append(\n",
    "            f\"Row count {len(df):,} outside expected range \"\n",
    "            f\"[{DATASET_CONFIG['expected_rows_min']:,}, {DATASET_CONFIG['expected_rows_max']:,}]\"\n",
    "        )\n",
    "    \n",
    "    # Check column count\n",
    "    if not (DATASET_CONFIG['expected_columns_min'] <= len(df.columns) <= DATASET_CONFIG['expected_columns_max']):\n",
    "        errors.append(\n",
    "            f\"Column count {len(df.columns)} outside expected range \"\n",
    "            f\"[{DATASET_CONFIG['expected_columns_min']}, {DATASET_CONFIG['expected_columns_max']}]\"\n",
    "        )\n",
    "    \n",
    "    # Check target column exists\n",
    "    if DATASET_CONFIG['target_column'] not in df.columns:\n",
    "        errors.append(f\"Target column '{DATASET_CONFIG['target_column']}' not found\")\n",
    "    \n",
    "    # Check for completely empty dataframe\n",
    "    if df.empty:\n",
    "        errors.append(\"DataFrame is completely empty\")\n",
    "    \n",
    "    # Check for all-null columns\n",
    "    all_null_cols = df.columns[df.isnull().all()].tolist()\n",
    "    if all_null_cols:\n",
    "        errors.append(f\"Columns with all null values: {all_null_cols}\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count > len(df) * 0.01:  # More than 1% duplicates\n",
    "        errors.append(f\"High duplicate count: {duplicate_count:,} rows ({duplicate_count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    is_valid = len(errors) == 0\n",
    "    \n",
    "    return is_valid, errors\n",
    "\n",
    "\n",
    "def analyze_data_quality(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform comprehensive data quality analysis.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing quality metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    quality_metrics = {\n",
    "        'basic_info': {\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns),\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024**2),\n",
    "            'duplicate_rows': df.duplicated().sum(),\n",
    "        },\n",
    "        'data_types': {\n",
    "            'numeric': len(df.select_dtypes(include=[np.number]).columns),\n",
    "            'object': len(df.select_dtypes(include=['object']).columns),\n",
    "            'datetime': len(df.select_dtypes(include=['datetime64']).columns),\n",
    "            'categorical': len(df.select_dtypes(include=['category']).columns),\n",
    "        },\n",
    "        'missing_values': {\n",
    "            'total_missing': df.isnull().sum().sum(),\n",
    "            'percentage': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,\n",
    "            'columns_with_missing': df.columns[df.isnull().any()].tolist(),\n",
    "        },\n",
    "        'target_distribution': None,\n",
    "    }\n",
    "    \n",
    "    # Analyze target variable if exists\n",
    "    if DATASET_CONFIG['target_column'] in df.columns:\n",
    "        target_col = DATASET_CONFIG['target_column']\n",
    "        value_counts = df[target_col].value_counts()\n",
    "        quality_metrics['target_distribution'] = {\n",
    "            'values': value_counts.to_dict(),\n",
    "            'default_rate': df[target_col].mean() if df[target_col].dtype in [int, float] else None,\n",
    "        }\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(filepath: Path, sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV data with optimized memory usage.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file\n",
    "        sample_size: Optional number of rows to sample (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        Loaded DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"‚è≥ Loading data from {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load with low_memory=False to prevent dtype warnings\n",
    "        if sample_size:\n",
    "            df = pd.read_csv(filepath, nrows=sample_size, low_memory=False)\n",
    "            print(f\"‚úì Loaded sample of {len(df):,} rows\")\n",
    "        else:\n",
    "            df = pd.read_csv(filepath, low_memory=False)\n",
    "            print(f\"‚úì Loaded {len(df):,} rows √ó {len(df.columns)} columns\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó Error: File not found at {filepath}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"‚úó Error: File is empty\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading data: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# METADATA MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def save_metadata(df: pd.DataFrame, quality_metrics: Dict):\n",
    "    \"\"\"\n",
    "    Save dataset metadata for reproducibility.\n",
    "    \n",
    "    Args:\n",
    "        df: Source DataFrame\n",
    "        quality_metrics: Quality analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        'dataset': {\n",
    "            'source': DATASET_CONFIG['url'],\n",
    "            'filename': RAW_DATA_PATH.name,\n",
    "            'download_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'md5_hash': calculate_md5(RAW_DATA_PATH) if RAW_DATA_PATH.exists() else None,\n",
    "        },\n",
    "        'dimensions': {\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns),\n",
    "            'memory_mb': round(df.memory_usage(deep=True).sum() / (1024**2), 2),\n",
    "        },\n",
    "        'columns': {\n",
    "            'names': df.columns.tolist(),\n",
    "            'dtypes': df.dtypes.astype(str).to_dict(),\n",
    "        },\n",
    "        'quality_metrics': quality_metrics,\n",
    "    }\n",
    "    \n",
    "    with open(METADATA_PATH, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n‚úì Metadata saved to: {METADATA_PATH}\")\n",
    "\n",
    "\n",
    "def print_data_summary(df: pd.DataFrame, quality_metrics: Dict):\n",
    "    \"\"\"\n",
    "    Print comprehensive data summary.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to summarize\n",
    "        quality_metrics: Quality metrics dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    print_section_header(\"DATA SUMMARY\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(\"üìä BASIC INFORMATION:\")\n",
    "    print(f\"  Rows: {quality_metrics['basic_info']['rows']:,}\")\n",
    "    print(f\"  Columns: {quality_metrics['basic_info']['columns']}\")\n",
    "    print(f\"  Memory: {quality_metrics['basic_info']['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"  Duplicates: {quality_metrics['basic_info']['duplicate_rows']:,}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã DATA TYPES:\")\n",
    "    print(f\"  Numeric: {quality_metrics['data_types']['numeric']}\")\n",
    "    print(f\"  Text/Object: {quality_metrics['data_types']['object']}\")\n",
    "    print(f\"  Datetime: {quality_metrics['data_types']['datetime']}\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nüîç MISSING VALUES:\")\n",
    "    print(f\"  Total: {quality_metrics['missing_values']['total_missing']:,}\")\n",
    "    print(f\"  Percentage: {quality_metrics['missing_values']['percentage']:.2f}%\")\n",
    "    print(f\"  Columns affected: {len(quality_metrics['missing_values']['columns_with_missing'])}\")\n",
    "    \n",
    "    # Target distribution\n",
    "    if quality_metrics['target_distribution']:\n",
    "        print(\"\\nüéØ TARGET DISTRIBUTION:\")\n",
    "        dist = quality_metrics['target_distribution']\n",
    "        for value, count in dist['values'].items():\n",
    "            pct = (count / quality_metrics['basic_info']['rows']) * 100\n",
    "            label = \"Fully Paid\" if value == 0 else \"Default\"\n",
    "            print(f\"  {label} ({value}): {count:,} ({pct:.2f}%)\")\n",
    "        \n",
    "        if dist['default_rate'] is not None:\n",
    "            print(f\"  Default Rate: {dist['default_rate']*100:.2f}%\")\n",
    "    \n",
    "    # Sample columns\n",
    "    print(f\"\\nüìù SAMPLE COLUMNS (first 10):\")\n",
    "    for i, col in enumerate(df.columns[:10], 1):\n",
    "        dtype = df[col].dtype\n",
    "        non_null = df[col].notna().sum()\n",
    "        print(f\"  {i:2d}. {col:30s} ({dtype}) - {non_null:,} non-null\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    print_section_header(\"PHASE 1: DATA LOADING & VALIDATION\")\n",
    "    \n",
    "    # Step 1: Setup\n",
    "    print(\"üîß Setting up directory structure...\")\n",
    "    ensure_directories()\n",
    "    \n",
    "    # Step 2: Download\n",
    "    success, error = download_data(\n",
    "        url=DATASET_CONFIG['url'],\n",
    "        filepath=RAW_DATA_PATH,\n",
    "        force=False\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"\\n‚úó Download failed: {error}\")\n",
    "        print(\"\\nüìã Manual download instructions:\")\n",
    "        print(f\"   1. Visit: {DATASET_CONFIG['url']}\")\n",
    "        print(f\"   2. Download file to: {RAW_DATA_PATH}\")\n",
    "        print(f\"   3. Re-run this script\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Step 3: Load\n",
    "    print_section_header(\"LOADING DATA\")\n",
    "    df = load_data(RAW_DATA_PATH)\n",
    "    \n",
    "    # Step 4: Validate\n",
    "    print_section_header(\"VALIDATING DATA\")\n",
    "    is_valid, errors = validate_data_structure(df)\n",
    "    \n",
    "    if not is_valid:\n",
    "        print(\"‚úó Data validation failed:\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"‚úì Data validation passed!\")\n",
    "    \n",
    "    # Step 5: Analyze quality\n",
    "    print(\"\\n‚è≥ Analyzing data quality...\")\n",
    "    quality_metrics = analyze_data_quality(df)\n",
    "    \n",
    "    # Step 6: Summary\n",
    "    print_data_summary(df, quality_metrics)\n",
    "    \n",
    "    # Step 7: Save metadata\n",
    "    save_metadata(df, quality_metrics)\n",
    "    \n",
    "    # Final message\n",
    "    print_section_header(\"‚úì PHASE 1 COMPLETE\")\n",
    "    print(\"Next steps:\")\n",
    "    print(\"  1. Review metadata: cat data/raw_data_metadata.json\")\n",
    "    print(\"  2. Run EDA: python src/2_exploratory_analysis.py\")\n",
    "    print(\"  3. Check quality report for any concerns\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
